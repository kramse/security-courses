\documentclass[a4paper,11pt,notitlepage]{report}
% Henrik Lund Kramshoej, February 2001
% hlk@security6.net,
% My standard packages
\usepackage{zencurity-network-exercises}

\begin{document}

\rm
\selectlanguage{english}

\newcommand{\subject}[1]{SIEM and Log Analysis course}

\mytitle{SIEM and Log Analysis}{exercises}

\pagenumbering{roman}


\setcounter{tocdepth}{0}

\normal

{\color{titlecolor}\tableofcontents}
%\listoffigures - not used
%\listoftables - not used

\normal
\pagestyle{fancyplain}
\chapter*{\color{titlecolor}Preface}
\markboth{Preface}{}

This material is prepared for use in \emph{\subject} and was prepared by
Henrik Kramselund Jereminsen, \link{http://www.zencurity.com} .
It describes the networking setup and
applications for trainings and courses where hands-on exercises are needed.

Further a presentation is used which is available as PDF from kramse@Github\\
Look for \jobname in the repo security-courses.

These exercises are expected to be performed in a training setting with network connected systems. The exercises use a number of tools which can be copied and reused after training. A lot is described about setting up your workstation in the repo

\link{https://github.com/kramse/kramse-labs}


\section*{\color{titlecolor}Prerequisites}

This material expect that participants have a working knowledge of
TCP/IP from a user perspective. Basic concepts such as web site addresses and email should be known as well as IP-addresses and common protocols like DHCP.

\vskip 1cm
Have fun and learn
\eject

% =================== body of the document ===============
% Arabic page numbers
\pagenumbering{arabic}
\rhead{\fancyplain{}{\bf \chaptername\ \thechapter}}

% Main chapters
%---------------------------------------------------------------------
% gennemgang af emnet
% check questions

\chapter*{\color{titlecolor}Exercise content}
\markboth{Exercise content}{}

Most exercises follow the same procedure and has the following content:
\begin{itemize}
\item {\bf Objective:} What is the exercise about, the objective
\item {\bf Purpose:} What is to be the expected outcome and goal of doing this exercise
\item {\bf Suggested method:} suggest a way to get started
\item {\bf Hints:} one or more hints and tips or even description how to
do the actual exercises
\item {\bf Solution:} one possible solution is specified
\item {\bf Discussion:} Further things to note about the exercises, things to remember and discuss
\end{itemize}

Please note that the method and contents are similar to real life scenarios and does not detail every step of doing the exercises. Entering commands directly from a book only teaches typing, while the exercises are designed to help you become able to learn and actually research solutions.


\chapter{Download Debian Administrator’s Handbook (DEB) Book 10 min}
\label{ex:sw-downloadDEB}

\hlkimage{3cm}{book-debian-administrators-handbook.jpg}


{\bf Objective:}\\
We need a Linux for running some tools during the course. I have chosen Debian Linux as this is open source, and the developers have released a whole book about running it.

This book is named
\emph{The Debian Administrator’s Handbook},  - shortened DEB

{\bf Purpose:}\\
We need to install Debian Linux in a few moments, so better have the instructions ready.

{\bf Suggested method:}\\
Create folders for educational materials. Go to download from the link \url{https://debian-handbook.info/}
Read and follow the instructions for downloading the book.

{\bf Solution:}\\
When you have a directory structure for download for this course, and the book DEB in PDF you are done.

{\bf Discussion:}\\
Linux is free and everywhere. The tools we will run in this course are made for Unix, so they run great on Linux.

Debian Linux is a free operating system platform.

The book DEB is free, but you can buy/donate to Debian, and I recommend it.

Not curriculum but explains how to use Debian Linux


\chapter{Check your Debian VM 10 min}
\label{ex:sw-basicDebianVM}

\hlkimage{7cm}{debian-xfce.png}

{\bf Objective:}\\
Make sure your virtual machine is in working order.

We need a Debian Linux for running tools during the course.

{\bf Purpose:}\\
If your VM is not installed and updated we will run into trouble later.

{\bf Suggested method:}\\
Go to \link{https://github.com/kramse/kramse-labs/}

Read the instructions for the setup of a Debian VM.

{\Large \bf This is a bonus exercise - only one Debian is needed per team.}

{\bf Hints:}\\
If you allocate enough memory and disk you wont have problems.

{\bf I suggest 50G disk, 2CPU cores and 6Gb memory for this course, if you have this.}

{\bf Solution:}\\
When you have a updated virtualisation software and a running VM, then we are good.

{\bf Discussion:}\\
Linux is free and everywhere. The tools we will run in this course are made for Unix, so they run great on Linux.

Debian Linux allows us to run Ansible and provision a whole SIEM in very few minutes.


\chapter{Investigate /etc 10 min}
\label{ex:sw-basicLinuxetc}


{\bf Objective:}\\
We will investigate the /etc directory on Linux. We need a Debian Linux

{\bf Purpose:}\\
Start seeing example configuration files, including:
\begin{itemize}
  \item User database \verb+/etc/passwd+ and \verb+/etc/group+
  \item The password database \verb+/etc/shadow+
\end{itemize}

{\bf Suggested method:}\\
Boot your Linux VMs, log in

Investigate permissions for the user database files \verb+passwd+ and \verb+shadow+

{\bf Hints:}\\
Linux has many tools for viewing files, the most efficient would be less.

\begin{alltt}
user@debian:~$ cd /etc
user@debian:/etc$ ls -l shadow passwd
-rw-r--r-- 1 root root   2203 Mar 26 17:27 passwd
-rw-r----- 1 root shadow 1250 Mar 26 17:27 shadow
user@debian:/etc$ ls
... all files and directories shown, investigate more if you like
\end{alltt}

Showing a single file: \verb+less /etc/passwd+ and press q to quit

Showing multiple files: \verb+less /etc/*+ then :n for next and q for quit

\begin{alltt}
Trying reading the shadow file as your regular user:
user@debian:/etc$ cat /etc/shadow
cat: /etc/shadow: Permission denied
\end{alltt}

Why is that? Try switching to root, using su or sudo, and redo the command.

{\bf Solution:}\\
When you have seen the most basic files you are done.

Also note the difference between running as root and normal user. Usually books and instructions will use a prompt of hash mark \verb+#+ when the root user is assumed and dollar sign \verb+$+ when a normal user prompt.

{\bf Discussion:}\\
Linux is free and everywhere. The tools we will run in this course are made for Unix, so they run great on Linux.

Sudo is a tool often used for allowing users to perform certain tasks as the super user. The tool is named from superuser do! \link{https://en.wikipedia.org/wiki/Sudo}


\chapter{Enable UFW firewall - 10 min}
\label{ex:debian-firewall}

{\bf Objective:}\\
Turn on a firewall and configure a few simple rules.

{\bf Purpose:}\\
See how easy it is to restrict incoming connections to a server.


{\bf Suggested method:}\\
Install a utility for firewall configuration.

You could also perform Nmap port scan with the firewall enabled and disabled.

{\bf Hints:}\\
Using the ufw package it is very easy to configure the firewall on Linux.

Install and configuration can be done using these commands.
\begin{alltt}
root@debian01:~# apt install ufw
Reading package lists... Done
Building dependency tree
Reading state information... Done
The following NEW packages will be installed:
  ufw
0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.
Need to get 164 kB of archives.
After this operation, 848 kB of additional disk space will be used.
Get:1 http://mirrors.dotsrc.org/debian stretch/main amd64 ufw all 0.35-4 [164 kB]
Fetched 164 kB in 2s (60.2 kB/s)
...
root@debian01:~# ufw allow 22/tcp
Rules updated
Rules updated (v6)
root@debian01:~# ufw enable
Command may disrupt existing ssh connections. Proceed with operation (y|n)? y
Firewall is active and enabled on system startup
root@debian01:~# ufw status numbered
Status: active

     To                         Action      From
     --                         ------      ----
[ 1] 22/tcp                     ALLOW IN    Anywhere
[ 2] 22/tcp (v6)                ALLOW IN    Anywhere (v6)
\end{alltt}

Also allow port 80/tcp and port 443/tcp - and install a web server. Recommend Nginx \verb+apt-get install nginx+

{\bf Solution:}\\
When firewall is enabled and you can still connect to Secure Shell (SSH) and web service, you are done.

{\bf Discussion:}\\
Further configuration would often require adding source prefixes which are allowed to connect to specific services. If this was a database server the database service should probably not be reachable from all of the Internet.

Web interfaces also exist, but are more suited for a centralized firewall.

Configuration of this firewall can be done using ansible, see the documentation and examples at \url{https://docs.ansible.com/ansible/latest/modules/ufw_module.html}

Should you have both a centralized firewall in front of servers, and local firewall on each server? Discuss within your team.




\chapter{Postman API Client 20 min}
\label{ex:postman-api}

%\hlkimage{10cm}{kali-linux.png}

{\bf Objective:}\\
Get a program capable of sending REST HTTP calls installed.


{\bf Purpose:}\\
Debugging REST is often needed, and some tools like Elasticsearch is both configured and maintained using REST APIs.

{\bf Suggested method:}\\
Download the app from
\link{https://www.postman.com/downloads/}

Available for Windows, Mac and Linux.

{\bf Hints:}\\
You can run the application without signing in anywhere.

{\bf Solution:}\\
When you have performed a REST call from within this tool, you are done.

Example: use the fake site \link{https://jsonplaceholder.typicode.com/todos/1} and other similar methods from the same (fake) REST API

If you have Elasticsearch installed and running try: \link{http://127.0.0.1:9200}

{\bf Discussion:}\\
Multiple applications and plugins can perform similar functions. This is a standalone app.

Tools like Elasticsearch has plugins allowing decoupling of the API and plugins. Example: \link{https://www.elastic.co/what-is/elasticsearch-monitoring} and \link{https://www.elastic.co/what-is/open-x-pack}



\chapter{Git tutorials - 15min}
\label{ex:git-tutorial}


\hlkimage{3cm}{git-logo.png}

{\bf Objective:}\\
Try the program Git locally on your workstation

{\bf Purpose:}\\
Running Git will allow you to clone repositories from others easily. This is a great way to get new software packages, and share your own.

Git is the name of the tool, and Github is a popular site for hosting git repositories.

{\bf Suggested method:}\\
Run the program from your Linux VM. You can also clone from your Windows or Mac OS X computer. Multiple graphical front-end programs exist too.

Most important are Git clone and pull:
\begin{alltt}\footnotesize
user@Projects:tt$ {\bf git clone https://github.com/kramse/kramse-labs.git}
Cloning into 'kramse-labs'...
remote: Enumerating objects: 283, done.
remote: Total 283 (delta 0), reused 0 (delta 0), pack-reused 283
Receiving objects: 100% (283/283), 215.04 KiB | 898.00 KiB/s, done.
Resolving deltas: 100% (145/145), done.

user@Projects:tt$ {\bf cd kramse-labs/}

user@Projects:kramse-labs$ {\bf ls}
LICENSE  README.md  core-net-lab  lab-network  suricatazeek  work-station
user@Projects:kramse-labs$ git pull
Already up to date.
\end{alltt}

{\bf Hints:}\\
Browse the Git tutorials on \link{https://git-scm.com/docs/gittutorial}\\
and \link{https://guides.github.com/activities/hello-world/}

We will not do the whole tutorials within 15 minutes, but get an idea of the command line, and see examples. Refer back to these tutorials when needed or do them at home.

Note: you don't need an account on Github to download/clone repositories, but having an acccount allows you to save repositories yourself and is recommended.

{\bf Solution:}\\
When you have tried the tool and seen the tutorials you are done.

{\bf Discussion:}\\
Before Git there has been a range of version control systems,\\
see \link{https://en.wikipedia.org/wiki/Version\_control} for more details.




\chapter{Getting started with the Elastic Stack - 60 min}
\label{gettingstartedelastic}

\hlkimage{10cm}{illustrated-screenshot-hero-kibana.png}


{\bf Objective:}\\
Get a working Elasticsearch, so we can do requests.

{\bf Purpose:}\\
We need some tools to demonstrate SIEM systems. Elasticsearch is a search engine and document store used in a lot of different systems, allowing cross application integration.

Elasticsearch uses REST extensively in their application.

{\bf Suggested method:}\\
Visit the web page for \emph{Getting started with the Elastic Stack} :\\
{\footnotesize\url{https://www.elastic.co/guide/en/elastic-stack-get-started/current/get-started-elastic-stack.html}}

Read about the tools, and the steps needed for manual installation.

When installing I highly recommend my Ansible based approach - which allows installation in less than 10 minutes automatically. It can also later be re-used in your own organizations to create a proof-of-concept or even production systems.

The ansible is described in exercise \ref{ex:basicansible} on page \pageref{ex:basicansible}

{\bf Hints:}\\
Elasticsearch can store almost anything we like.

The web page for the getting started show multiple sections:
\begin{itemize}
\item Elasticsearch - the core engine, this must be done manually or with Ansible
\item Kibana - the analytics and visualization platform
\item Beats - data shippers, a way to get some data into ES
\item Logstash (optional) offers a large selection of plugins to help you parse, enrich, transform, and buffer data from a variety of sources
\end{itemize}

Each describes a part and are recommended reading.


{\bf Solution:}\\
When you have browsed the page you are done.

{\bf Discussion:}\\
We could have used a lot of other servers and service, which ones would you prefer?



\chapter{Use Ansible to install Elastic Stack}
\label{ex:basicansible}


{\bf Objective:}\\
Run Elasticsearch

{\bf Purpose:}\\
See an example tool used for many projects, Elasticsearch from the Elastic Stack

{\bf Suggested method:}\\
We will run Elasticsearch, either using the method from:\\{\footnotesize
\url{https://www.elastic.co/guide/en/elastic-stack-get-started/current/get-started-elastic-stack.html}}

or by the method described below using Ansible - your choice.

Ansible used below is a configuration management tool \url{https://www.ansible.com/} and you can adjust them for production use!

I try to test my playbooks using both Ubuntu and Debian Linux, but Debian is the main target for this training.

First make sure your system is updated, as root run:

\begin{minted}[fontsize=\footnotesize]{shell}
apt-get update && apt-get -y upgrade && apt-get -y dist-upgrade
\end{minted}

You should reboot if the kernel is upgraded :-)

Second make sure your system has ansible and my playbooks: (as root run)
\begin{minted}[fontsize=\footnotesize]{shell}
apt -y install ansible git python
git clone https://github.com/kramse/kramse-labs
\end{minted}

We will run the playbooks locally, while a normal Ansible setup would use SSH to connect to the remote node.

Then it should be easy to run Ansible playbooks, like this: (again as root, most packet sniffing things will need root too later)

\begin{minted}[fontsize=\footnotesize]{shell}
cd kramse-labs/suricatazeek
ansible-playbook -v 1-dependencies.yml 2-suricatazeek.yml 3-elasticstack.yml
\end{minted}

Note: I keep these playbooks flat and simple, but you should investigate Ansible roles for real deployments.

If I update these, it might be necessary to update your copy of the playbooks. Run this while you are in the cloned repository:

\begin{minted}[fontsize=\footnotesize]{shell}
git pull
\end{minted}

Note: usually I would recommend running git clone as your personal user, and then use sudo command to run some commands as root. In a training environment it is OK if you want to run everything as root. Just beware.

Note: these instructions are originally from the course\\
Go to \url{https://github.com/kramse/kramse-labs/tree/master/suricatazeek}

{\bf Hints:}\\
Ansible is great for automating stuff, so by running the playbooks we can get a whole lot of programs installed, files modified - avoiding the Vi editor \smiley

Example playbook content, installing software using APT:
\begin{minted}[fontsize=\footnotesize]{yaml}
apt:
    name: "{{ packages }}"
    vars:
      packages:
        - nmap
        - curl
        - iperf
        ...
\end{minted}

{\bf Solution:}\\
When you have a updated VM and Ansible running, then we are good.

{\bf Discussion:}\\
Linux is free and everywhere. The tools we will run in this course are made for Unix, so they run great on Linux.


\chapter{Making requests to Elasticsearch - 15-75min}
\label{ex:es-rest-api}

%\hlkimage{10cm}{kali-linux.png}

{\bf Objective:}\\
Use APIs for accessing Elasticsearch data, both internal and user data.

{\bf Purpose:}\\
Learn how to make requests to an API.

{\bf Suggested method:}\\
Go to the list of exposed Elasticsearch REST APIs:\\
\link{https://www.elastic.co/guide/en/elasticsearch/reference/current/rest-apis.html}

The Elasticsearch REST APIs are exposed using JSON over HTTP.

Select a category example, Cluster APIs, then select Nodes Info APIs. This will show URLs you can use:

\begin{minted}[fontsize=\footnotesize]{shell}
# return just process
curl -X GET "localhost:9200/\_nodes/process?pretty"
# same as above
curl -X GET "localhost:9200/\_nodes/_all/process?pretty"

curl -X GET "localhost:9200/_nodes/plugins?pretty"

# return just jvm and process of only nodeId1 and nodeId2
curl -X GET "localhost:9200/\_nodes/nodeId1,nodeId2/jvm,process?pretty"
# same as above
curl -X GET "localhost:9200/\_nodes/nodeId1,nodeId2/info/jvm,process?pretty"
# return all the information of only nodeId1 and nodeId2
curl -X GET "localhost:9200/\_nodes/nodeId1,nodeId2/_all?pretty"
\end{minted}

When you can see this works, then feel free to install X-Pack and monitoring plugins

{\bf Hints:}\\
Pretty Results can be obtained using the pretty parameter.
\begin{quote}
When appending ?pretty=true to any request made, the JSON returned will be pretty formatted (use it for debugging only!). Another option is to set ?format=yaml which will cause the result to be returned in the (sometimes) more readable yaml format.
\end{quote}

Lots of tutorials exist for accessing Elasticsearch

A couple of examples:
\begin{itemize}
\item \link{https://aws.amazon.com/blogs/database/elasticsearch-tutorial-a-quick-start-guide/}
\item \link{https://www.digitalocean.com/community/tutorials/how-to-install-elasticsearch-logstash-and-kibana-elastic-stack-on-ubuntu-18-04}
\end{itemize}

{\bf Solution:}\\
When you have seen examples of the API, understand the references with underscore, like \verb+_nodes+ and pretty printing you are done.

I recommend playing with Elasticsearch plugins and X-pack.\\
\link{https://www.elastic.co/downloads/x-pack}

Note: In versions 6.3 and later, X-Pack is included with the default distributions of Elastic Stack, with all free features enabled by default.

Also Kibana can be used for creating nice dashboards and become applications more or less.

{\bf Discussion:}\\
You can also try calling the REST API from Python

Similar to this:
\inputminted{python}{programs/rest-1.py}




% 1-overview starts here




\chapter{Use a XML library in Python up to 30min}
\label{ex-python-library}

{\bf Objective:}\\
Try using a programing library in the Python programming language.

{\bf Purpose:}\\
See how easy it is to produce functionality by re-using existing functions and features available in a popular language.

{\bf Suggested method:}\\
Start by getting an XML file. Suggested method is to boot your Linux and run a command like \verb+nmap -p 80,443 -A -oA testfile www.zencurity.com+. Output should be \verb+testfile.xml+ and two other files, grepable output \verb+testfile.gnmap+ and text output \verb+testfile.nmap+.

Then using Python import a library to parse XML and print a few values from the XML, or all of them.

Recommended values to print from the file:
\begin{list2}
\item Nmap version
\item Date of the Nmap run, note either use start and convert from Unix time or startstr which is a string
\item Nmaprun args - aka the command line
\item Host address
\item Ports like from the <port protocol="tcp" portid="443">
\item Anything you feel like
\end{list2}

{\bf Hints:}\\
One option is to use the Python ElementTree XML API:\\
\url{https://docs.python.org/3/library/xml.etree.elementtree.html}

Also - use Python3!

{\bf Solution:}\\
When you can read a file and process it using Python3.

Improvements, you might consider:
\begin{list2}
\item Use Python3 to run the Nmap process
\item Create command line parameters for the program, making it more useful
\item Pretty print using formatted output
\end{list2}
{\bf Discussion:}\\
Many examples contain code like this:

\begin{quote}
Getting child tag's attribute value in a XML using ElementTree

Parse the XML file and get the root tag and then using [0] will give us first child tag. Similarly [1], [2] gives us subsequent child tags. After getting child tag use \verb+.attrib[attribute_name]+ to get value of that attribute.
\end{quote}
\begin{minted}[fontsize=\footnotesize]{python}
>>> import xml.etree.ElementTree as ET
>>> xmlstr = '<foo><bar key="value">text</bar></foo>'
>>> root = ET.fromstring(xmlstr)
>>> root.tag
'foo'
>>> root[0].tag
'bar'
>>> root[0].attrib['key']
'value'
\end{minted}
Source:\\{\footnotesize \url{https://stackoverflow.com/questions/4573237/how-to-extract-xml-attribute-using-python-elementtree}}

What is the point of referring to a specific numbered child, when we specifically have the tags?!

\chapter{Clone a Python library StevenBlack/hosts - 30min}
\label{ex:git-clone-XX}

{\bf Objective:}\\
Find an interesting library on Github, suggest the one called:

\link{https://github.com/StevenBlack/hosts}

{\bf Purpose:}\\
Being able to find libraries on Github can make your life easier.

We will use an example that can download \verb+hosts+ files, what is a hosts file?
{\bf Suggested method:}\\
Clone the repository in your Linux VM

Browse the readme.md

{\bf Hints:}\\
Python package manager \verb+pip+ makes it easy to install dependencies for a program.

Install the dependencies with:

\begin{minted}[fontsize=\footnotesize]{shell}
pip3 install --user -r requirements.txt
\end{minted}


{\bf Solution:}\\
When you have read about the tool you are done.

We don't need to run the program today, but feel free to do so.

{\bf Discussion:}\\
Are these sources recommendable?

Make sure you read the license, read about the program, possibly inspect the source code.

AND data downloaded also often have restrictions


\chapter{Date Formats 15 min}
\label{ex:dateformats}

%\hlkimage{10cm}{kali-linux.png}

{\bf Objective:}\\
See an example of time parsing, and realize how difficult time can be.

{\bf Purpose:}\\
System integration often works with different representations of the same data. Time and dates are one aspect we often meet. Realize how complex it is.

{\bf Suggested method:}\\
Visit the web pages of an existing tool, Logstash we will use througout the course and a standard for time and dates.

{\bf Write down todays date on a piece of paper, each one does their own.}

Then lookup ISO 8601\\
\url{https://en.wikipedia.org/wiki/ISO_8601}

I recommend looking at a specific system, used for processing computer logs:
Logstash \\
\url{https://www.elastic.co/guide/en/logstash/current/plugins-filters-date.html}


{\bf Hints:}\\
When you receive a date there are so many formats, that you need to be very specific how to interpret it.

Parsing dates is a complex task, best left for existing frameworks and functions.

If you decide to parse dates using your own code, then centralize it - so you can update it when you find bugs.

{\bf Solution:}\\
When you have a logstash reading a date, or via a Grok debugger on the web

{\bf Discussion:}\\
Make sure to visit the web page:\\
\url{https://infiniteundo.com/post/25326999628/falsehoods-programmers-believe-about-time}

Did you realize how complex time and computers are?

Then consider this software bug:\\
"No, you're not crazy. Open Office can't print on Tuesdays."\\
\url{https://bugs.launchpad.net/ubuntu/+source/file/+bug/248619}

Linked from\\
{\footnotesize\url{https://www.reddit.com/r/linux/comments/9hdam/no_youre_not_crazy_open_office_cant_print_on/}}

Because a command \verb+file+ has an error in parsing data, files with PostScript data - print jobs with the text Tue - are interpreted as being Erlang files instead. This breaks the printing, on Tue(sdays).

We will go through this bug in detail together.


\chapter{Bonus: Grok Debugger 30 min}
\label{ex:grokdebugger1}

\hlkimage{10cm}{grok-debugger.png}

{\bf Objective:}\\
Try parsing dates using an existing system.

{\bf Purpose:}\\
See how existing systems can support advanced parsing, without programming.

{\bf Suggested method:}\\
Go to the web application Grok Debugger:\\
\url{https://grokdebug.herokuapp.com/}

Try entering data into the input field, and a parsing expression in the Pattern field.

Try the data from
\url{https://www.elastic.co/guide/en/kibana/current/xpack-grokdebugger.html}



{\bf Hints:}\\
The expression with greedy data is nice for matching a lot of text:\\
\verb+%{GREEDYDATA:message}+

Try adding some text at the end of the input, and another part of the parsing with this.

{\bf Solution:}\\
When you have parsed a line and seen it you are done.

{\bf Discussion:}\\
The functionality Grok debugging is included in the tool Kibana from Elastic:\\
\url{https://www.elastic.co/guide/en/kibana/current/xpack-grokdebugger.html}




% 2-hello-world starts here

\chapter{Data types -- IP addresses 15min}
\label{ex:data-types-ip-address}

{\bf Objective:}\\
Find out what IP-addresses really are -- just a 32-bit integer for IPv4.


{\bf Purpose:}\\
When working with IP addresses, it can be more efficient to use math, than string matching!


{\bf Suggested method:}\\
Lets visit the DDS book, page 73 and onwards. Quoted here for ease of use:


\begin{quote}
Since you know an 8-bit byte can range in value from 0 to 255, you also know the dotted-decimal range is 0.0.0.0 through 255.255.255.255, which is 32 bits. If you count the possible address space, you have a total of 4,294,967,296 possible addresses (the maximum value of a 32-bit integer). This brings up another point of storing and handling IP addresses: {\bf Any IP address can be converted to/from a 32-bit integer value.}

This is important because the integer representation saves both space and time and you can calculate some things a bit easier with that representation than with the dotted-decimal form. If you are writing or using a tool that perceives an IP address only as a character string or as a set of character strings, you are potentially wasting space by trading a 4-byte, 32-bit representation for a 15-byte, 120-bit representation (worst case).

Furthermore, you are also choosing to use less efficient string comparison code versus integer arithmetic and comparison plus bitwise operations to accomplish the same tasks. Although this may have little to no impact in some scenarios, the repercussions grow significant when you’re dealing with large volumes of IP addresses (and become worse in the IPv6 world) and repeated operations.
\end{quote}
Source: \emph{Data-Driven Security: Analysis, Visualization and Dashboards} Jay Jacobs, Bob Rudis  ISBN: 978-1-118-79372-5


{\bf Hints:}\\
When working with IP addresses always use libraries

{\bf Solution:}\\
When you have pinged the IPv4 address of 2130706433 you are done.
\verb+ping 2130706433+


{\bf Discussion:}\\



\chapter{Data types -- IP reputation 15min}
\label{siem:ip-reputation}

{\bf Objective:}\\
Find out what IP reputation lists are with some examples.


{\bf Purpose:}\\
Identifying bad things can be hard.

We have a concept named, Indicators of Compromise (IoC).

Indicators of Compromise (IOC) any piece of information that can be used to objectively describe a network intrusion, expressed in a platform-independent manner. Say, if a server connects to THIS specific IP, which is KNOWN to be the control and command server for a malware strain, we conclude the device is infected.

For this we can often download files for identification purposes with some reputation.

{\bf Suggested method:}\\
We will start with the example of
AlienVault’s IP Reputation database

Note: links change, and the link in the book does not work!

\link{http://reputation.alienvault.com/reputation.data}

\link{https://rdrr.io/github/hrbrmstr/netintel/man/Alien.Vault.Reputation.html}


{\bf Hints:}\\
Passive DNS systems exist, which allow you to lookup older records, things that have moved.

Maltrail software contains a lot of lists\\
\link{https://github.com/stamparm/maltrail}

{\bf Solution:}\\
When you have understood that data from others can help your identification efforts, you are done.

{\bf Discussion:}\\
We also have used reputation a lot in fighting spam and scam emails.



\chapter{Bonus: Zeek on the web 10min}
\label{ex:zeekweb}


{\bf Objective:} \\
Try Zeek Network Security Monitor - without installing it.


{\bf Purpose:}\\
Show a couple of examples of Zeek scripting, the built-in language found in Zeek Network Security Monitor


{\bf Suggested method:}\\
Go to \url{http://try.bro.org/#/?example=hello}

{\bf Hints:}\\
The exercise
\emph{The Summary Statistics Framework} can be run with a specifc PCAP.

\verb+192.168.1.201 did 402 total and 2 unique DNS requests in the last 6 hours.+

{\bf Solution:}\\
You should read the example \emph{Raising a Notice}. Getting output for certain events may be interesting to you.


{\bf Discussion:}\\
Zeek Network Security Monitor is an old/mature tool, but can still be hard to get started using. I would suggest that you always start out using the packages available in your Ubuntu/Debian package repositories.

They work, and will give a first impression of Zeek. If you later want specific features not configured into the binary packet, then install from source.

Also Zeek uses a broctl program to start/stop the tool, and a few config files which we should look at. From a Debian system they can be found in \verb+/etc/bro+ :

\begin{minted}[fontsize=\footnotesize]{shell}
root@NMS-VM:/etc/bro# ls -la
drwxr-xr-x   3 root root  4096 Oct  8 08:36 .
drwxr-xr-x 138 root root 12288 Oct  8 08:36 ..
-rw-r--r--   1 root root  2606 Oct 30  2015 broctl.cfg
-rw-r--r--   1 root root   225 Oct 30  2015 networks.cfg
-rw-r--r--   1 root root   644 Oct 30  2015 node.cfg
drwxr-xr-x   2 root root  4096 Oct  8 08:35 site
\end{minted}

\chapter{Zeek DNS capturing domain names 15min}
\label{ex:zeekdnsbasic}


{\bf Objective:} \\
We will now start using Zeek on our systems.


{\bf Purpose:}\\
Try Zeek with example traffic, and see what happens.


{\bf Suggested method packet capture file:}\\
Use Nitroba.pcap can be found in various places around the internet

\begin{minted}[fontsize=\footnotesize]{shell}
$ cd
$ wget http://downloads.digitalcorpora.org/corpora/scenarios/2008-nitroba/nitroba.pcap
$ mkdir $HOME/bro;cd $HOME/bro; bro -r ../nitroba.pcap
... bro reads the packets
~/bro$ ls
conn.log  dns.log  dpd.log  files.log  http.log  packet_filter.log
sip.log  ssl.log  weird.log  x509.log
$ less *
\end{minted}

Use :n to jump to the next file in less, go through all of them.

{\bf Suggested method Live traffic:}\\
Make sure Zeek is configured as a standalone probe and configured for the right interface. Linux used to use eth0 as the first ethernet interface, but now can use others, like ens192 or enx00249b1b2991.

\begin{minted}[fontsize=\footnotesize]{shell}
root@NMS-VM:/etc/bro# cat node.cfg
# Example BroControl node configuration.
#
# This example has a standalone node ready to go except for possibly changing
# the sniffing interface.

# This is a complete standalone configuration.  Most likely you will
# only need to change the interface.
[bro]
type=standalone
host=localhost
interface=eth0
...
\end{minted}


{\bf Hints:}\\
There are multiple commands for showing the interfaces and IP addresses on Linux. The old way is using \verb+ifconfig -a+ newer systems would use \verb+ip a+

Note: if your system has a dedicated interface for capturing, you need to turn it on, make it available. This can be done manually using \verb+ifconfig eth0 up+
{\bf Solution:}\\
When you either run Zeek using a packet capture or using live traffic

Running with a capture can be done using a command line such as:
\verb+bro -r traffic.pcap+

Using broctl to start it would be like this:
\begin{minted}[fontsize=\footnotesize]{shell}
// install bro first
kunoichi:~ root# broctl
Hint: Run the broctl "deploy" command to get started.

Welcome to BroControl 1.5
Type "help" for help.

[BroControl] > install
creating policy directories ...
installing site policies ...
generating standalone-layout.bro ...
...
\end{minted}

Afterwards you can stop and start as you wish:
\begin{minted}[fontsize=\footnotesize]{shell}
// back to Broctl and start it
[BroControl] > start
starting bro
[BroControl] > exit
// and then
kunoichi:bro root# cd /var/log/bro/current
kunoichi:bro root# tail -f dns.log
\end{minted}

You should be able to spot entries like this:
\begin{minted}[fontsize=\footnotesize]{shell}
#fields ts      uid     id.orig_h       id.orig_p       id.resp_h       id.resp_p       proto
trans_id        rtt query   qclass  qclass_name     qtype   qtype_name      rcode   rcode_name
AA      TC      RD      RA      Z       answers TTLs    rejected
1538982372.416180 CD12Dc1SpQm42QW4G3 10.xxx.0.145 57476 10.x.y.141 53	udp
20383	0.045021	www.dr.dk	1	C_INTERNET	1 A 0 NOERROR
F F T T 0 www.dr.dk-v1.edgekey.net,e16198.b.akamaiedge.net,2.17.212.93 60.000000,20409.000000,20.000000 F
\end{minted}

Note: this show ALL the fields captured and dissected by Zeek, there is a nice utility program bro-cut which can select specific fields:

\begin{minted}[fontsize=\footnotesize]{shell}
root@NMS-VM:/var/spool/bro/bro# cat dns.log | bro-cut -d ts query answers | grep dr.dk
2018-10-08T09:06:12+0200 www.dr.dk www.dr.dk-v1.edgekey.net,e16198.b.akamaiedge.net,2.17.212.93
\end{minted}

{\bf Discussion:}\\
Why is DNS interesting?


\chapter{Zeek TLS capturing certificates 15min}
\label{ex:zeektlsbasic}


{\bf Objective:} \\
Run more traffic through Zeek, see the various files.


{\bf Purpose:}\\
See that even though HTTPS and TLS traffic is encrypted it often show names and other values from the certificates and servers.


{\bf Suggested method:}\\
Run Zeek capturing live traffic, start https towards some sites. A lot of common sites today has shifted to HTTPS/TLS.


{\bf Hints:}\\
use broctl start and watch the output directory

\begin{alltt}\small
root@NMS-VM:/var/spool/bro/bro# ls *.log
communication.log  dhcp.log files.log known_services.log packet_filter.log  stats.log
stdout.log x509.log conn.log dns.log known_hosts.log loaded_scripts.log  ssl.log
stderr.log weird.log
\end{alltt}

We already looked at \verb+dns.log+, now check \verb+ssl.log+ and \verb+x509.log+

{\bf Solution:}\\
When you have multiple log files with data from Zeek, and have looked into some of them. You are welcome to ask questions and look into more files.


{\bf Discussion:}\\
How can you hide that you are going to HTTPS sites?

Hint: VPN




\chapter{Find Indices in Elasticsearch 15 min}
\label{ex:es-find-indices}

{\bf Objective:}\\
Find the indices in your Elasticsearch

Note: you might not have any.

{\bf Purpose:}\\
We need to locate our data. Data in Elasticsearch is saved in {\bf indices}.


{\bf Suggested method:}\\
Make sure Elasticsearch is running, with Kibana.

If you have Elasticsearch installed and running try: \link{http://127.0.0.1:9200}

\begin{minted}[fontsize=\footnotesize]{shell}
$ curl 127.0.0.1:9200
\end{minted}
should output:
\begin{minted}[fontsize=\footnotesize]{json}
{
  "name" : "debian-siem",
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "Kyi6e2WuSGq2TcFzOaPnsQ",
  "version" : {
    "number" : "7.10.0",
    "build_flavor" : "default",
    "build_type" : "deb",
    "build_hash" : "51e9d6f22758d0374a0f3f5c6e8f3a7997850f96",
    "build_date" : "2020-11-09T21:30:33.964949Z",
    "build_snapshot" : false,
    "lucene_version" : "8.7.0",
    "minimum_wire_compatibility_version" : "6.8.0",
    "minimum_index_compatibility_version" : "6.0.0-beta1"
  },
  "tagline" : "You Know, for Search"
}
\end{minted}


If you have Elasticsearch installed and running try: \link{http://127.0.0.1:5601}

Should show a basic Kibana window.
\hlkimage{7cm}{kibana-es-default.png}


If this is your first time in kibana, then you will probably see it asking for an index pattern:
\hlkimage{7cm}{kibana-default-index-pattern.png}

This is a short two-step process, and I choose \verb+filebeat-*+ as I wanted to play with filebeat data, and already had some available.

\hlkimage{10cm}{kibana-filebeat-pattern-1.png}

Make sure to select a time field:

\hlkimage{10cm}{kibana-filebeat-pattern-2.png}

When you have done your first index pattern, note the menu on the left, hamburger icon menu.

From there you can choose: \verb+Stack Management > Index Management+.

\hlkimage{10cm}{kibana-index-mgmt.png}

Feel free to delete data, or otherwise change data in your own installation.



{\bf Hints:}\\
Documentation is available at:\\
\link{https://www.elastic.co/guide/en/elasticsearch/reference/7.10/index-mgmt.html}

{\bf Solution:}\\
When you have ES and Kibana running, and can access the interfaces, you are done.

Before you leave, make note of the menu: Kibana Index Patterns. These are used for presenting data in Kibana, and you want them to show your data!

\begin{quote}
Kibana requires an index pattern to access the Elasticsearch data that you want to explore. An index pattern selects the data to use and allows you to define properties of the fields.
\end{quote}
Source: \link{https://www.elastic.co/guide/en/kibana/current/index-patterns.html}

So if you are running filebeat, and data from filebeat, then it is advised to use an index pattern of \verb+filebeat-*+

\hlkimage{10cm}{kibana-index-pattern.png}

You can create more of these, and common ones would be:
\begin{itemize}
\item \verb+filebeat-*+ -- all sorts of data ingested by filebeat
\item \verb+logstash-*+ -- all sorts of data ingested by logstash

\end{itemize}

{\bf Discussion:}\\
Elasticsearch is a huge system, watching tutorials and playing with the tools are the recommended way to learn.



\chapter{Zeek Data in Elasticsearch 30 min}
\label{ex:zeek-json-es}

{\bf Objective:}\\
Get data from Zeek files into Elasticsearch.


{\bf Purpose:}\\
Having the files with data is great, but to make it more accessible we will load it into ES.


{\bf Suggested method:}\\
Git pull the kramse-labs repository and do this automatically.

You can also do the steps manually, following a guide like:\\
\link{https://www.elastic.co/blog/collecting-and-analyzing-zeek-data-with-elastic-security}

\link{https://github.com/kramse/kramse-labs}

If you haven't clone, do this first in your Linux VM:
\begin{minted}[fontsize=\footnotesize]{shell}
apt -y install ansible git python
git clone https://github.com/kramse/kramse-labs
\end{minted}

Output should be similar to this:
\begin{minted}[fontsize=\footnotesize]{shell}
user@Projects:t$ git clone https://github.com/kramse/kramse-labs
Cloning into 'kramse-labs'...
remote: Enumerating objects: 283, done.
remote: Total 283 (delta 0), reused 0 (delta 0), pack-reused 283
Receiving objects: 100% (283/283), 215.04 KiB | 906.00 KiB/s, done.
Resolving deltas: 100% (145/145), done.
user@Projects:t$
\end{minted}

If you already cloned, get the latest updates:
\begin{minted}[fontsize=\footnotesize]{shell}
user@Projects:kramse-labs$ git pull
Already up to date.
user@Projects:kramse-labs$
\end{minted}

Run the playbooks for setting up the system {\bf as root}:
\begin{minted}[fontsize=\footnotesize]{shell}
cd kramse-labs/suricatazeek
sudo ansible-playbook -v 1-dependencies.yml 2-suricatazeek.yml 3-elasticstack.yml
\end{minted}

After this, you should have some data in ES/Kibana

Use the discover data menu:
\hlkimage{3cm}{kibana-discover.png}


{\bf Hints:}\\
Make sure you use the right index pattern. Within discover you can change between them with the menu:

\hlkimage{3cm}{kibana-change-index-pattern.png}

Another hint, for lab systems that are turned off and on, use the time selector at the top and select "Last 90 days". This way you can see older data you imported last time. Production systems continually produce data, but your lab server probably has very little.

\hlkimage{7cm}{kibana-90-day-lab.png}

{\bf Solution:}\\
When you have tried the filebeat tool and seen some data you are done.

{\bf Discussion:}\\






\end{document}





\chapter{Wireshark 15 min}
\label{ex:wireshark-install}

\hlkimage{10cm}{wireshark-http.png}


{\bf Objective:}\\
Try the program Wireshark locally your workstation, or tcpdump

You can run Wireshark on your host too, if you want.

{\bf Purpose:}\\
Installing Wireshark will allow you to analyse packets and protocols

See real network traffic, also know that a lot of information is available and not encrypted.

Note the three way handshake between hosts running TCP. You can either use a browser or command line tools like cURL while capturing

\begin{alltt}
curl http://www.zencurity.com
\end{alltt}


{\bf Suggested method:}\\
Run Wireshark from your Linux

Open Wireshark and start a capture\\
Then in another window execute the ping program while sniffing

or perform a Telnet connection while capturing data

{\bf Hints:}\\
PCAP is a packet capture library allowing you to read packets from the network.
Tcpdump uses libpcap library to read packet from the network cards and save them.
Wireshark is a graphical application to allow you to browse through traffic, packets and protocols.

If not already on your Linux, or do: \verb+apt-get install wireshark+

When running on Linux the network cards are usually named eth0 for the first Ethernet and wlan0 for the first Wireless network card. In Windows the names of the network cards are long and if you cannot see which cards to use then try them one by one.

{\bf Solution:}\\
When you have collected some HTTP/TCP sessions you are done.

If you want to capture packets as a non-root user on Debian, then use the command to add a Wireshark group:
\begin{alltt}
sudo dpkg-reconfigure wireshark-common
\end{alltt}

and add your user to this:
\begin{alltt}
sudo gpasswd -a $USER wireshark
\end{alltt}
Dont forget to logout/login to pick up this new group.

{\bf Discussion:}\\
Wireshark is just an example other packet analyzers exist, some commercial and some open source like Wireshark

We can download a lot of packet traces from around the internet, we might use examples from\\
\link{https://old.zeek.org/community/traces.html}




\chapter{Sniff Your Browser 15min}
\label{ex:sniff-captive-portal}


{\bf Objective:}\\
See an example of a simple network application behaviour.

{\bf Purpose:}\\
Learn how to get started analysing network application traffic.

{\bf Suggested method:}\\
Modern browser check if they are online by making requests.

Which requests does a browser make by itself, even though you haven't entered URL yet?

Use Wireshark on your Linux or normal operating system. Start your capture, start your browser.

See if you can identify the traffic.

{\bf Hints:}\\
You should be looking for DNS and HTTP/HTTPS requests.

DNS uses port 53/udp and 53/tcp.

Also googling captive portal and Firefox reveals a setting you can turn of or on.

You might also have observed this when you proxied your browser through Burp suite in an earlier exercise.

{\bf Solution:}\\
When you have identified the traffic belonging to at least one browser you are done. Firefox should be easy.

{\bf Discussion:}\\
Does initiating this from a browser have privacy implications?

Your internet provider can see when you are home, when you start your browser etc. Requests made are often with a lot of extra information, like User-Agent and distinguishable.

Example, my son uses an iPhone, but I use an Android. One user might use Windows 7, while another uses Windows 10 - traffic will be different.





\chapter{ xx min}
\label{ex:}

{\bf Objective:}\\
Try the program XX locally your workstation


{\bf Purpose:}\\
Running XXX will allow you to analyse


\begin{alltt}


\end{alltt}


{\bf Suggested method:}\\
Run the program from your Linux VM


{\bf Hints:}\\

{\bf Solution:}\\
When you have tried the tool and seen some data you are done.

{\bf Discussion:}\\




\end{document}
