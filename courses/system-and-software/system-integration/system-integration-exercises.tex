\documentclass[a4paper,11pt,notitlepage]{report}
% Henrik Lund Kramshoej, February 2001
% hlk@security6.net,
% My standard packages
\usepackage{zencurity-network-exercises}
\usepackage{minted}

\begin{document}

\rm
\selectlanguage{english}

\newcommand{\course}[1]{System Integration F2020}

\mytitle{System Integration F2020}{Exercises}

\pagenumbering{roman}
\setcounter{tocdepth}{0}

\normal

{\color{titlecolor}\tableofcontents}
%\listoffigures - not used
%\listoftables - not used

\normal
\pagestyle{fancyplain}
\chapter*{\color{titlecolor}Preface}
\markboth{Preface}{}

This material is prepared for use in \emph{\course} and was prepared by
Henrik Lund Kramshoej, Zencurity Aps.
It describes the setup and
applications for trainings and workshops where hands-on exercises are needed.

\vskip 1cm
Further a presentation is used which is available as PDF from kramse@Github\\
Look for \jobname in the repo security-courses.

These exercises are expected to be performed in a training setting with network
connected systems. The exercises use a number of tools which can be copied and
reused after training. A lot is described about setting up your workstation in
the repo

\link{https://github.com/kramse/kramse-labs}

\section*{\color{titlecolor}Prerequisites}

This material expect that participants have a working knowledge of internet from
a user perspective. Basic concepts such as web site addresses, IP-addresses and
email should be known as well.

\vskip 1cm
Have fun and learn
\eject

% =================== body of the document ===============
% Arabic page numbers
\pagenumbering{arabic}
\rhead{\fancyplain{}{\bf \chaptername\ \thechapter}}

% Main chapters
%---------------------------------------------------------------------
% gennemgang af emnet
% check questions

\chapter*{\color{titlecolor}Exercise content}
\markboth{Exercise content}{}

Most exercises follow the same procedure and has the following content:
\begin{itemize}
\item {\bf Objective:} What is the exercise about, the objective
\item {\bf Purpose:} What is to be the expected outcome and goal of doing this exercise
\item {\bf Suggested method:} suggest a way to get started
\item {\bf Hints:} one or more hints and tips or even description how to
do the actual exercises
\item {\bf Solution:} one possible solution is specified
\item {\bf Discussion:} Further things to note about the exercises, things to remember and discuss
\end{itemize}

Please note that the method and contents are similar to real life scenarios and does not detail every step of doing the exercises. Entering commands directly from a book only teaches typing, while the exercises are designed to help you become able to learn and actually research solutions.


\chapter{Date Formats 15 min}
\label{ex:sys-int-dateformats}

%\hlkimage{10cm}{kali-linux.png}

{\bf Objective:}\\
See an example of time parsing, and realize how difficult time can be in system integration.

{\bf Purpose:}\\
System integration often works with different representations of the same data. Time and dates are one aspect we often meet. Realize how complex it is.

{\bf Suggested method:}\\
Visit the web pages of an existing tool, Logstash we will use througout the course and a standard for time and dates.

{\bf Write down todays date on a piece of paper, each one does their own.}

Then lookup ISO 8601\\
\url{https://en.wikipedia.org/wiki/ISO_8601}

I recommend looking at a specific system, used for processing computer logs:
Logstash \\
\url{https://www.elastic.co/guide/en/logstash/current/plugins-filters-date.html}


{\bf Hints:}\\
When you receive a date there are so many formats, that you need to be very specific how to interpret it.

Parsing dates is a complex task, best left for existing frameworks and functions.

If you decide to parse dates using your own code, then centralize it - so you can update it when you find bugs.

{\bf Solution:}\\
When you have a logstash reading a date, or via a Grok debugger on the web

{\bf Discussion:}\\
Make sure to visit the web page:\\
\url{https://infiniteundo.com/post/25326999628/falsehoods-programmers-believe-about-time}

Did you realize how complex time and computers are?

Then consider this software bug:\\
"No, you're not crazy. Open Office can't print on Tuesdays."\\
\url{https://bugs.launchpad.net/ubuntu/+source/file/+bug/248619}

Linked from\\
{\footnotesize\url{https://www.reddit.com/r/linux/comments/9hdam/no_youre_not_crazy_open_office_cant_print_on/}}

Because a command \verb+file+ has an error in parsing data, files with PostScript data - print jobs with the text Tue - are interpreted as being Erlang files instead. This breaks the printing, on Tue(sdays).

We will go through this bug in detail together.


\chapter{Grok Debugger 15 min}
\label{ex:grokdebugger1}

\hlkimage{10cm}{grok-debugger.png}

{\bf Objective:}\\
Try parsing dates using an existing system.

{\bf Purpose:}\\
See how existing systems can support advanced parsing, without programming.

{\bf Suggested method:}\\
Go to the web application Grok Debugger:\\
\url{https://grokdebug.herokuapp.com/}

Try entering data into the input field, and a parsing expression in the Pattern field.

Try the data from
\url{https://www.elastic.co/guide/en/kibana/current/xpack-grokdebugger.html}



{\bf Hints:}\\
The expression with greedy data is nice for matching a lot of text:\\
\verb+%{GREEDYDATA:message}+

Try adding some text at the end of the input, and another part of the parsing with this.

{\bf Solution:}\\
When you have parsed a line and seen it you are done.

{\bf Discussion:}\\
The functionality Grok debugging is included in the tool Kibana from Elastic:\\
\url{https://www.elastic.co/guide/en/kibana/current/xpack-grokdebugger.html}


\chapter{Getting started with the Elastic Stack 15 min}
\label{gettingstartedelastic}

\hlkimage{10cm}{illustrated-screenshot-hero-kibana.png}

Screenshot from \url{https://www.elastic.co/kibana}

{\bf Objective:}\\
Get ready to start using Elasticsearch, read - but dont install.

{\bf Purpose:}\\
We need some tools to demonstrate integration. Elasticsearch is a search engine and ocument store used in a lot of different systems, allowing cross application integration.


{\bf Suggested method:}\\
Visit the web page for \emph{Getting started with the Elastic Stack} :\\
{\footnotesize\url{https://www.elastic.co/guide/en/elastic-stack-get-started/current/get-started-elastic-stack.html}}

Read about the tools, and the steps needed for manual installation.

{\bf You dont need to install the tools currently}, I recommend using Debian and Ansible for bringing up Elasticsearch.
You are of course welcome to install, or try the Docker method.


{\bf Hints:}\\
Elasticsearch is the name of the search engine and document store. Today Elastic Stack contains lots of different parts.

We will focus on these parts:
\begin{itemize}
\item Elasticsearch - the core engine
\item Logstash - a tool for parsing logs and other data.\\
\url{https://www.elastic.co/logstash}
\begin{quote}
"Logstash dynamically ingests, transforms, and ships your data regardless of format or complexity. Derive structure from unstructured data with grok, decipher geo coordinates from IP addresses, anonymize or exclude sensitive fields, and ease overall processing."
\end{quote}
\item Kibana - a web application for accessing and working with data in Elasticsearch\\
\url{https://www.elastic.co/kibana}
\end{itemize}

{\bf Solution:}\\
When you have browsed the page you are done.

{\bf Discussion:}\\
You can read more about Elasticsearch at the wikipedia page:\\
\url{https://en.wikipedia.org/wiki/Elasticsearch}

\chapter{Check your Debian VM 10 min}
\label{ex:basicDebianVM}

\hlkimage{10cm}{debian-xfce.png}

{\bf Objective:}\\
Make sure your virtual Debian machine is in working order.
We need a Debian 10 Linux for running a few extra tools during the course.

{\Large \bf This is a bonus exercise - only one Debian is needed per team.}

{\bf Purpose:}\\
If your VM is not installed and updated we will run into trouble later.

{\bf Suggested method:}\\
Go to \link{https://github.com/kramse/kramse-labs/} Read the instructions for the setup of a Debian VM.

%{\bf Hints:}\\

{\bf Solution:}\\
When you have a updated virtualisation software and Debian Linux, then we are good.
Create a snapshot of the server, so you can return to this, in case it breaks later.

{\bf Discussion:}\\
Linux is free and everywhere. The tools we will run in this course are made for Unix, so they run great on Linux.

Even Microsoft has made their cloud Linux friendly, and post articles about creating Linux applications:\\
\url{https://docs.microsoft.com/en-us/azure/security/develop/}



\chapter{Use Ansible to install Elastic Stack}
\label{ex:basicansible}


{\bf Objective:}\\
Run Elasticsearch

{\bf Purpose:}\\
See an example tool used for many integration projects, Elasticsearch from the Elastic Stack

{\bf Suggested method:}\\
We will run Elasticsearch, either using the method from:\\{\footnotesize
\url{https://www.elastic.co/guide/en/elastic-stack-get-started/current/get-started-elastic-stack.html}}

or by the method described below using Ansible - your choice.

Ansible used below is a configuration management tool \url{https://www.ansible.com/}

I try to test my playbooks using both Ubuntu and Debian Linux, but Debian is the main target for this training.

First make sure your system is updated, as root run:

\begin{minted}[fontsize=\footnotesize]{shell}
apt-get update && apt-get -y upgrade && apt-get -y dist-upgrade
\end{minted}

You should reboot if the kernel is upgraded :-)

Second make sure your system has ansible and my playbooks: (as root run)
\begin{minted}[fontsize=\footnotesize]{shell}
apt -y install ansible git
git clone https://github.com/kramse/kramse-labs
\end{minted}

We will run the playbooks locally, while a normal Ansible setup would use SSH to connect to the remote node.

Then it should be easy to run Ansible playbooks, like this: (again as root, most packet sniffing things will need root too later)

\begin{minted}[fontsize=\footnotesize]{shell}
cd kramse-labs/suricatazeek
ansible-playbook -v 1-dependencies.yml 2-suricatazeek.yml 3-elasticstack.yml
\end{minted}

Note: I keep these playbooks flat and simple, but you should investigate Ansible roles for real deployments.

If I update these, it might be necessary to update your copy of the playbooks. Run this while you are in the cloned repository:

\begin{minted}[fontsize=\footnotesize]{shell}
git pull
\end{minted}

Note: usually I would recommend running git clone as your personal user, and then use sudo command to run some commands as root. In a training environment it is OK if you want to run everything as root. Just beware.

Note: these instructions are originally from the course\\
Go to \url{https://github.com/kramse/kramse-labs/tree/master/suricatazeek}

{\bf Hints:}\\
Ansible is great for automating stuff, so by running the playbooks we can get a whole lot of programs installed, files modified - avoiding the Vi editor \smiley

Example playbook content
\begin{alltt}
apt:
      name: "{{ packages }}"
    vars:
      packages:
        - nmap
        - curl
        - iperf
        ...
\end{alltt}

{\bf Solution:}\\
When you have a updated VM and Ansible running, then we are good.

{\bf Discussion:}\\
Linux is free and everywhere. The tools we will run in this course are made for Unix, so they run great on Linux.


\chapter{Run Nginx as a load balancer}
\label{ex:nginx-loadbalancer}


{\bf Objective:}\\
Run Nginx in a load balancing configuration.

{\bf Purpose:}\\
See an example load balancing tool used for many integration projects, Nginx

{\bf Suggested method:}\\
Running Nginx as a load balancer does not require a lot of configuration.

First goal: Make Nginx listen on two ports by changing the default configuration.

\begin{list2}
\item Start by installing Nginx in your Debian, see it works - open localhost port 80 in browser\\
\verb+apt install nginx+
\item Copy the configuration file! Keep this backup,\\
\verb+cd /etc/nginx/;cp nginx.conf nginx.conf.orig+
\item Add / copy the section for the port 80 server, see below
\item Change \emph{sites} to use port 81 and port 82
\end{list2}


Creating a new site, based on the default site found on Nginx in Debian:
\begin{minted}[fontsize=\footnotesize]{shell}
root@debian-lab:/etc/nginx# cd /etc/nginx/sites-enabled/
root@debian-lab:/etc/nginx/sites-enabled# cp default default2
root@debian-lab:/etc/nginx/sites-enabled# cd /var/www/
root@debian-lab:/var/www# cp -r html html2
\end{minted}

Then edit files \verb+default+ to use port 81/tcp and \verb+default2+ to use port 82/tcp

- also make sure \verb+default2+ uses \verb+root /var/www/html2+

Configuration changes made.

These are the changes you should make:
\begin{minted}[fontsize=\footnotesize]{shell}
root@debian-lab:/etc/nginx/sites-enabled# diff default default2
22,23c22,23
< 	listen 81 default_server;
< 	listen [::]:81 default_server;
---
> 	listen 82 default_server;
> 	listen [::]:82 default_server;
41c41
< 	root /var/www/html;
---
> 	root /var/www/html2;
\end{minted}



Config test and restart of Nginx can be done using stop and start commands:

\begin{minted}[fontsize=\footnotesize]{shell}
root@debian-lab:/var/www# nginx -t
nginx: the configuration file /etc/nginx/nginx.conf syntax is ok
nginx: configuration file /etc/nginx/nginx.conf test is successful
root@debian-lab:/var/www# service nginx stop
root@debian-lab:/var/www# service nginx start
\end{minted}

You can now visit \url{http://127.0.0.1:81} and \url{http://127.0.0.1:82}\\
 - which show the same text, but you can change the files in \\
 \verb+/var/www/html+ and \verb+/var/www/html2+

NOTE: also verify that port 80 does \emph{not work} anymore!

Adding the loadbalancer in nginx.conf.

We can now add the two \emph{servers} running into a single loadbalancer with a little configuration:

Add this into \verb+/etc/nginx/nginx.conf+ - inside the section \verb+http { ... }+
\begin{minted}[fontsize=\footnotesize]{shell}
upstream myapp1 {
        server localhost:81;
        server localhost:82;
}

server {
        listen 80;

        location / {
                proxy_pass http://myapp1;
        }
}
\end{minted}

And test using \url{http://127.0.0.1:80}



{\bf Hints:}\\
Make changes to the two sets of HTML files

\begin{minted}[fontsize=\footnotesize]{shell}
root@debian-lab:~# cd /var/www/
root@debian-lab:/var/www# diff html
html/  html2/
root@debian-lab:/var/www# diff html/index.nginx-debian.html html2/index.nginx-debian.html
4c4
< <title>Welcome to nginx!</title>
---
> <title>Welcome to nginx2!</title>
14c14
< <h1>Welcome to nginx!</h1>
---
> <h1>Welcome to nginx2!</h1>
\end{minted}

When reloading the page a few times it will switch between the two versions

{\bf Solution:}\\
When you have Nginx running load balanced, then we are good.

{\bf Discussion:}\\
Nginx is one of the most popular load balancers and web servers. Many sites use it for processing HTTPS/TLS before reaching application servers.







\chapter{Runing ActiveMQ}
\label{ex:activemq-install}

\hlkimage{15cm}{activemq-web_console.png}

{\bf Objective:}\\
Try running ActiveMQ manually - outside of Camel

{\bf Purpose:}\\
System integration often works with JMS and ActiveMQ implements this.
We can run ActiveMQ as part of Camel, but lets try running it alone.

{\bf Suggested method:}\\
Visit the web page \url{https://activemq.apache.org/getting-started}, read and also follow download link.

I used \verb+apache-activemq-5.15.11-bin.tar.gz+

Follow the instructions for getting ActiveMQ up and running on your Debian server.

Best course of actions is to use a root user and directory such as \verb+/opt+ - something like this:

\begin{alltt}
cd /opt
tar zxvf /directory/where/you/downloaded/
mkdir data;chmod a+rwx data/
./bin/activemq console
\end{alltt}


{\bf Hints:}\\
ActiveMQ is also available as a package in Debian - try searching with \verb+apt search activemq+
and can be installed with \verb+apt install activemq+

Note: when installing the package maintainers version the configuration files are often found in the directories below \verb+/etc+ while running an unpacked tgz from the project they are close to the software. YMMV.

{\bf It is not recommended to use the Debian package for this exercise, but for production use it would be.}

In this case there is a configuration file available, but not activated, try this:\\
\verb+sudo ln -s /etc/activemq/instances-available/main /etc/activemq/instances-enabled/main+

When changing configuration files, if using the Debian package, you can get debug info:
\verb+/etc/init.d/activemq console main+

One problem I observed was the directory missing, which can be created using this:
\begin{alltt}
# mkdir -p  /var/lib/activemq/data/
# chmod a+rwx  /var/lib/activemq/data/
\end{alltt}

afterwards when programs drop files, we can check the settings, ownership and tighten this.

{\bf Also this configuration does NOT start the web console!}

{\bf Solution:}\\
When you have a running ActiveMQ and can see the web administration you are done.

If you want to investigate further get the demos up and running:\\
\url{https://activemq.apache.org/web-samples}

{\bf Discussion:}\\
What are the benefits of running ActiveMQ from Debian package vs running it from project binaries directly?

What version does your application need? How do you guarantee this?







\chapter{Runing RabbitMQ with Python}
\label{ex:rabbitmq-install}

%\hlkimage{15cm}{activemq-web_console.png}

{\bf Objective:}\\
Try running RabbitMQ with a few Python programs.

{\bf Purpose:}\\
RabbitMQ is an alternative to ActiveMQ.

{\bf Suggested method:}\\
Use the RabbitMQ tutorial to send a message. Use the tutorial:\\
\url{https://www.rabbitmq.com/tutorials/tutorial-one-python.html}

First you would install the server and suporting library - Pika:
\begin{alltt}
apt install rabbitmq-server python-pika
\end{alltt}

\eject
Check status:
\begin{alltt}
# service rabbitmq-server status
● rabbitmq-server.service - RabbitMQ Messaging Server
   Loaded: loaded (/lib/systemd/system/rabbitmq-server.service; enabled; vendor
   Active: active (running) since Sun 2020-03-08 13:57:38 CET; 36s ago
 Main PID: 1663 (beam.smp)
   Status: "Initialized"
    Tasks: 87 (limit: 2386)
   Memory: 77.9M
   CGroup: /system.slice/rabbitmq-server.service
           ├─1659 /bin/sh /usr/sbin/rabbitmq-server
           ├─1663 /usr/lib/erlang/erts-10.2.4/bin/beam.smp -W w -A 64 -MBas agef
           ├─1898 erl_child_setup 65536
           ├─1917 inet_gethost 4
           └─1918 inet_gethost 4

Mar 08 13:57:34 elastilab systemd[1]: Starting RabbitMQ Messaging Server...
Mar 08 13:57:38 elastilab systemd[1]: rabbitmq-server.service: Supervising proce
Mar 08 13:57:38 elastilab systemd[1]: Started RabbitMQ Messaging Server.
Mar 08 13:57:38 elastilab systemd[1]: rabbitmq-server.service: Supervising proce
root@elastilab:~#
\end{alltt}


Sender:
\inputminted{python}{programs/sender.py}

Receiver:
\inputminted{python}{programs/recv.py}

\eject
{\bf Hints:}\\
Running receiver would look like this:
\begin{alltt}
kramse@elastilab:~/projects/rabbit$ ./recv.py
 [*] Waiting for messages. To exit press CTRL+C
 [x] Received 'Hello World!'
 [x] Received 'Hello World!'
 [x] Received 'Hello World!'
\end{alltt}



{\bf Solution:}\\
When you have a running RabbitMQ with the Python programs working you are done.

{\bf Discussion:}\\
RabbitMQ supports many libraries and languages, check out the list on:
\url{https://www.rabbitmq.com/devtools.html}

What if you want to connect ActiveMQ and RabbitMQ?

Both suppoprt AMQP 1.0 - so this might be a way to support this.

\chapter{Runing OpenJDK for Camel}
\label{ex:openjdk8}

%\hlkimage{15cm}{activemq-web_console.png}

{\bf Objective:}\\
Install OpenJDK 8 from AdoptOpenJDK on your Debian server.

{\bf Purpose:}\\
Get OpenJDK 8 installed for the exercises to work.

{\bf Suggested method:}\\
First clone the instructors camelinaction2 repository

\begin{alltt}
git clone https://github.com/kramse/camelinaction2.git
\end{alltt}

This will download a version of the book files, where pom.xml is updated to include modules needed for them to work. Use this directory when running mvn commands from the book.


Then use the instructions from this article, to install OpenJDK 8\\
\url{https://linuxize.com/post/install-java-on-debian-10}

Then make this your default java - as root:
\begin{alltt}
# cd /usr/bin
# mv java java.orig        //  Make sure we can go back to OpenJDK 11 later
# ln -s /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64/bin/java java
\end{alltt}

This saves the old Java link, and creates a new link to the just installed OpenJDK 11.

{\bf Hints:}\\
If the changes made work, you can run the OpenJDK 8:
\begin{alltt}
  hlk@debian-lab:~$ java -version
openjdk version "1.8.0_242"
OpenJDK Runtime Environment (AdoptOpenJDK)(build 1.8.0_242-b08)
OpenJDK 64-Bit Server VM (AdoptOpenJDK)(build 25.242-b08, mixed mode)
\end{alltt}



{\bf Solution:}\\
When your java version command gives the 1.8 it works.


{\bf Discussion:}\\
The book examples use spring and JAXB and things that aren't available in the newer Java JDK versions.

Also the pom.xml for Maven needs references updated.





\chapter{Run PostgreSQL}
\label{ex:postgresql-tutorial}

\hlkimage{10cm}{postgresql-short.png}

{\bf Objective:}\\
Try a real SQL RDBMS.

{\bf Purpose:}\\
Relational databases are used around the world for storing production data. When doing system integration projects we will often need to read or store data in databases, so a minimum of knowledge about these are needed.


{\bf Suggested method:}\\
First visit \url{https://en.wikipedia.org/wiki/Relational_database}
and read about relational database systems RDBMS.

Visit the home page of PostgreSQL \url{https://www.postgresql.org/} and read a little about this project. How mature is this, what version is current, would you run this in production?

Then go to the tutorials listed on:
\url{https://www.postgresqltutorial.com/}

Perform the following as a minimum:
\begin{list2}
\item Section 1. Getting Started with PostgreSQL\\
See note about Debian below. I recommend running it on Debian
\item Section 2. Querying Data
\item Section 3. Filtering Data
\item Section 13. Managing Databases, parts Create, Alter, Rename, Drop, Copy
\end{list2}



{\bf Hints:}\\
Note: Debian already has PostgreSQL in the package system, it is recommended to use the version "Included in distribution"\\ \url{https://www.postgresql.org/download/linux/debian/}

\begin{alltt}
apt-get install postgresql-11
\end{alltt}

{\bf Solution:}\\
When you have installed a PostgreSQL system and can do queries against a sample database you are done. I recommend familiarizing yourself with the basic SQL statement select with a where clause.

{\bf Discussion:}\\
Who would choose PostgreSQL, and who would use Microsoft SQL?

Why choose Oracle SQL?


\chapter{Why go to SOA 45 min}
\label{ex:why-soa}

%\hlkimage{10cm}{kali-linux.png}

{\bf Objective:}\\
Consider why SOA is a good idea.

After reading chapters 1-5 in the SOA book


{\bf Purpose:}\\
Think about when to use SOA, and when other models might be sufficient.

{\bf Suggested method:}\\
Open your favourite presentation program.

Create a one-page, and one-page only, presentation about SOA in \emph{Your Company}

{\bf Hints:}\\
Consider this is a presentation for the management and CEO of a company. The company might have various challenges which SOA can help with. Distill the knowledge from the chapters into short sentences, bullets etc.

An idea is to split the page into fields 4-9 boxes and have small headlines "What is", "Why SOA", "current challenges", "goals" and similar.

{\bf Solution:}\\
When you have a one-page presentation that summs up why you should use SOA you are done. I would like to see them, but the main goal of this exercise is to consider SOA as a tool, and know when to use it.

{\bf Discussion:}\\
Instead of SOA your company might have used industry standard tools like JSON, XML, REST, WSDL etc. but is it enough to ensure future interoperability?

Notes: The above reference to the SOA book, is the book:\\
\emph{Service-Oriented Architecture: Analysis and Design for Services and Microservices},\\ Thomas Erl, 2017
ISBN: 978-0-13-385858-7



\chapter{Cloud Computing Introduction 45 min}
\label{ex:cloud-intro}

%\hlkimage{10cm}{kali-linux.png}

{\bf Objective:}\\
Many businesses today use cloud services, but what are they?!

{\bf Purpose:}\\
Have minimum knowledge about the term cloud computing.

{\bf Suggested method:}\\
Read \url{https://en.wikipedia.org/wiki/Cloud_computing}

Take special notice of the relation to client--server and mainframe which has been used a lot before, and often represent the systems to integrate with cloud.

Then read a little about two cloud systems:
\begin{list2}
\item Kubernetes \url{https://en.wikipedia.org/wiki/Kubernetes}\\
 and the homepage \url{https://kubernetes.io/}
\item Microsoft Azure \url{https://en.wikipedia.org/wiki/Microsoft_Azure}
\end{list2}

{\bf Hints:}\\
Some people run K8S inside Azure, why? To allow them to move applications from their on-site / on-premise cloud into the Azure cloud on the internet easily.

{\bf Solution:}\\
You are done, when you have read the main cloud computing wikipedia page. You should have an idea about cloud computing and the various terms Platform as a service (PaaS) and Software as a service (SaaS).

{\bf Discussion:}\\
Other cloud systems exist, and today system integration projects often have a part with cloud integration.

\chapter{Cloud Deployment 45 min}
\label{ex:azure-secure-app}

%\hlkimage{10cm}{kali-linux.png}

{\bf Objective:}\\
See how you would deploy an application into a cloud, using the Microsoft Azure cloud example
\emph{Develop a secure web app}



{\bf Purpose:}\\
See some steps needed to produce real applications in the cloud.

{\bf Suggested method:}\\
Go to and read: \url{https://docs.microsoft.com/en-us/azure/security/develop/secure-web-app}

You are not expected to work through the example, but get an idea about how you deploy applications securely in the modern internet.

{\bf Hints:}\\
Security is a huge part of running computing today, but often forgotten.

The example both describes the architecture of the applications, the security implications, and the actual deployment of an app.

{\bf Solution:}\\
When you have read the example you are done.

{\bf Discussion:}\\
If you want to work with cloud technologies you would often use Azure, but also other clouds like Amazon Web Services (AWS) are popular in Denmark.

Read more about AWS in wikipedia:\\
\url{https://en.wikipedia.org/wiki/Amazon_Web_Services}




\chapter{Download the Microservices ebook 20 min}
\label{ex:download-microservices-book}

%\hlkimage{10cm}{kali-linux.png}

{\bf Objective:}\\
Download the book: \emph{Microservices for Java Developers}
by Christian Posta
{\bf Purpose:}\\

{\bf Suggested method:}\\
Download the book via the link from:\\
\url{https://www.oreilly.com/programming/free/files/microservices-for-java-developers.pdf}

Update, currently link does not work. Will put file in Fronter!

{\bf Hints:}\\

{\bf Solution:}\\
When you have a copy of the PDF you are done.

{\bf Discussion:}\\
You are welcome to read chapter 1 of the book.




\chapter{Postman API Client 20 min}
\label{ex:postman-api}

%\hlkimage{10cm}{kali-linux.png}

{\bf Objective:}\\
Get a program capable of sending REST HTTP calls installed.


{\bf Purpose:}\\
Debugging REST is often needed, and some tools like Elasticsearch is both configured and maintained using REST APIs.

{\bf Suggested method:}\\
Download the app from
\link{https://www.postman.com/downloads/}

Available for Windows, Mac and Linux.

{\bf Hints:}\\
Download the Linux 64-bit version on your Debian.

Unpack using something like:
\begin{alltt}
cd ~;mkdir bin;cd bin
tar zxvf ~/Downloads/Postman-linux*
cd Postman;./Postman
\end{alltt}

You can run the application without signing in anywhere.

{\bf Solution:}\\
When you have performed a REST call from within this tool, you are done.

Example: use the fake site \link{https://jsonplaceholder.typicode.com/todos/1} and other similar methods from the same (fake) REST API

If you have Elasticsearch installed and running try: \link{http://127.0.0.1:9200}

{\bf Discussion:}\\
Multiple applications and plugins can perform similar functions. This is a standalone app.

Tools like Elasticsearch has plugins allowing decoupling of the API and plugins. Example: \link{https://www.elastic.co/what-is/elasticsearch-monitoring} and \link{https://www.elastic.co/what-is/open-x-pack}





\chapter{Install the Elastic Stack - 60 min}
\label{instalelastic}

\hlkimage{10cm}{illustrated-screenshot-hero-kibana.png}


{\bf Objective:}\\
Get a working Elasticsearch, so we can do requests.

{\bf Purpose:}\\
Elasticsearch uses REST extensively in their application.

{\bf Suggested method:}\\
either use the
\emph{Getting started with the Elastic Stack}
\link{https://www.elastic.co/guide/en/elastic-stack-get-started/current/get-started-elastic-stack.html}

OR my Ansible based approach - which some already ran.

The ansible is described in exercise \ref{ex:basicansible} on page \pageref{ex:basicansible}

{\bf Hints:}\\
We dont really need a lot in the Elasticsearch database, and you can run most tasks with zero data. Graphs will not be as pretty though.

{\bf Solution:}\\
When you have a running Elasticsearch you are done, and ready for next exercise.

The web page for the getting started show multiple sections:
\begin{itemize}
\item Elasticsearch - the core engine, this must be done manually or with Ansible
\item Kibana - the analytics and visualization platform
\item Beats - data shippers, a way to get some data into ES
\item Logstash (optional) offers a large selection of plugins to help you parse, enrich, transform, and buffer data from a variety of sources
\end{itemize}

Each describes a part and are recommended reading.

{\bf Discussion:}\\
We could have used a lot of other servers and service, which ones would you prefer?

If you have access to Azure, you can try Azure REST API Reference\\ \link{https://docs.microsoft.com/en-us/rest/api/azure/}

\chapter{Making requests to Elasticsearch - 15-75min}
\label{ex:es-rest-api}

%\hlkimage{10cm}{kali-linux.png}

{\bf Objective:}\\
Use APIs for accessing Elasticsearch data, both internal and user data.

{\bf Purpose:}\\
Learn how to make requests to an API.

{\bf Suggested method:}\\
Go to the list of exposed Elasticsearch REST APIs:\\
\link{https://www.elastic.co/guide/en/elasticsearch/reference/current/rest-apis.html}

The Elasticsearch REST APIs are exposed using JSON over HTTP.

Select a category example, Cluster APIs, then select Nodes Info APIs. This will show URLs you can use:

\begin{minted}[fontsize=\footnotesize]{shell}
# return just process
curl -X GET "localhost:9200/\_nodes/process?pretty"
# same as above
curl -X GET "localhost:9200/\_nodes/_all/process?pretty"

curl -X GET "localhost:9200/_nodes/plugins?pretty"

# return just jvm and process of only nodeId1 and nodeId2
curl -X GET "localhost:9200/\_nodes/nodeId1,nodeId2/jvm,process?pretty"
# same as above
curl -X GET "localhost:9200/\_nodes/nodeId1,nodeId2/info/jvm,process?pretty"
# return all the information of only nodeId1 and nodeId2
curl -X GET "localhost:9200/\_nodes/nodeId1,nodeId2/_all?pretty"
\end{minted}

When you can see this works, then feel free to install X-Pack and monitoring plugins

{\bf Hints:}\\
Pretty Results can be obtained using the pretty parameter.
\begin{quote}
When appending ?pretty=true to any request made, the JSON returned will be pretty formatted (use it for debugging only!). Another option is to set ?format=yaml which will cause the result to be returned in the (sometimes) more readable yaml format.
\end{quote}

Lots of tutorials exist for accessing Elasticsearch

A couple of examples:
\begin{itemize}
\item \link{https://aws.amazon.com/blogs/database/elasticsearch-tutorial-a-quick-start-guide/}
\item \link{https://www.digitalocean.com/community/tutorials/how-to-install-elasticsearch-logstash-and-kibana-elastic-stack-on-ubuntu-18-04}
\end{itemize}

{\bf Solution:}\\
When you have seen examples of the API, understand the references with underscore, like \verb+_nodes+ and pretty printing you are done.

I recommend playing with Elasticsearch plugins and X-pack.\\
\link{https://www.elastic.co/downloads/x-pack}

Note: In versions 6.3 and later, X-Pack is included with the default distributions of Elastic Stack, with all free features enabled by default.

Also Kibana can be used for creating nice dashboards and become applications more or less.

{\bf Discussion:}\\
You can also try calling the REST API from Python

Similar to what we did previously in this course:
\inputminted{python}{programs/rest-1.py}



\chapter{Find Indices in Elasticsearch 15 min}
\label{ex:es-find-indices}

{\bf Objective:}\\
Find the indices in your Elasticsearch

Note: you might not have any.

{\bf Purpose:}\\
We need to locate our data. Data in Elasticsearch is saved in {\bf indices}.


{\bf Suggested method:}\\
Make sure Elasticsearch is running, with Kibana.

If you have Elasticsearch installed and running try: \link{http://127.0.0.1:9200}

\begin{minted}[fontsize=\footnotesize]{shell}
$ curl 127.0.0.1:9200
\end{minted}
should output:
\begin{minted}[fontsize=\footnotesize]{json}
{
  "name" : "debian-siem",
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "Kyi6e2WuSGq2TcFzOaPnsQ",
  "version" : {
    "number" : "7.10.0",
    "build_flavor" : "default",
    "build_type" : "deb",
    "build_hash" : "51e9d6f22758d0374a0f3f5c6e8f3a7997850f96",
    "build_date" : "2020-11-09T21:30:33.964949Z",
    "build_snapshot" : false,
    "lucene_version" : "8.7.0",
    "minimum_wire_compatibility_version" : "6.8.0",
    "minimum_index_compatibility_version" : "6.0.0-beta1"
  },
  "tagline" : "You Know, for Search"
}
\end{minted}


If you have Elasticsearch installed and running try: \link{http://127.0.0.1:5601}

Should show a basic Kibana window.
\hlkimage{7cm}{kibana-es-default.png}


If this is your first time in kibana, then you will probably see it asking for an index pattern:
\hlkimage{7cm}{kibana-default-index-pattern.png}

This is a short two-step process, and I choose \verb+filebeat-*+ as I wanted to play with filebeat data, and already had some available.

\hlkimage{10cm}{kibana-filebeat-pattern-1.png}

Make sure to select a time field:

\hlkimage{10cm}{kibana-filebeat-pattern-2.png}

When you have done your first index pattern, note the menu on the left, hamburger icon menu.

From there you can choose: \verb+Stack Management > Index Management+.

\hlkimage{10cm}{kibana-index-mgmt.png}

Feel free to delete data, or otherwise change data in your own installation.



{\bf Hints:}\\
Documentation is available at:\\
\link{https://www.elastic.co/guide/en/elasticsearch/reference/7.10/index-mgmt.html}

{\bf Solution:}\\
When you have ES and Kibana running, and can access the interfaces, you are done.

Before you leave, make note of the menu: Kibana Index Patterns. These are used for presenting data in Kibana, and you want them to show your data!

\begin{quote}
Kibana requires an index pattern to access the Elasticsearch data that you want to explore. An index pattern selects the data to use and allows you to define properties of the fields.
\end{quote}
Source: \link{https://www.elastic.co/guide/en/kibana/current/index-patterns.html}

So if you are running filebeat, and data from filebeat, then it is advised to use an index pattern of \verb+filebeat-*+

\hlkimage{10cm}{kibana-index-pattern.png}

You can create more of these, and common ones would be:
\begin{itemize}
\item \verb+filebeat-*+ -- all sorts of data ingested by filebeat
\item \verb+logstash-*+ -- all sorts of data ingested by logstash

\end{itemize}

{\bf Discussion:}\\
Elasticsearch is a huge system, watching tutorials and playing with the tools are the recommended way to learn.


\chapter{Finding Data in Elasticsearch 30 min}
\label{ex:zeek-json-es}

{\bf Objective:}\\
Get data from Zeek files into Elasticsearch.

{\bf OR use the sample data, easily imported into ES}

{\bf Purpose:}\\
Having files with data is great, but to make it more accessible we will load it into ES.


{\bf Suggested method:}\\
Git pull the kramse-labs repository and do this automatically.

You can also do the steps manually, following a guide like:\\
\link{https://www.elastic.co/blog/collecting-and-analyzing-zeek-data-with-elastic-security}

\link{https://github.com/kramse/kramse-labs}

If you haven't clone, do this first in your Linux VM:
\begin{minted}[fontsize=\footnotesize]{shell}
apt -y install ansible git python
git clone https://github.com/kramse/kramse-labs
\end{minted}

Output should be similar to this:
\begin{minted}[fontsize=\footnotesize]{shell}
user@Projects:t$ git clone https://github.com/kramse/kramse-labs
Cloning into 'kramse-labs'...
remote: Enumerating objects: 283, done.
remote: Total 283 (delta 0), reused 0 (delta 0), pack-reused 283
Receiving objects: 100% (283/283), 215.04 KiB | 906.00 KiB/s, done.
Resolving deltas: 100% (145/145), done.
user@Projects:t$
\end{minted}

If you already cloned, get the latest updates:
\begin{minted}[fontsize=\footnotesize]{shell}
user@Projects:kramse-labs$ git pull
Already up to date.
user@Projects:kramse-labs$
\end{minted}

Run the playbooks for setting up the system {\bf as root}:
\begin{minted}[fontsize=\footnotesize]{shell}
cd kramse-labs/suricatazeek
sudo ansible-playbook -v 1-dependencies.yml 2-suricatazeek.yml 3-elasticstack.yml
\end{minted}

After this, you should have some data in ES/Kibana

Use the discover data menu:
\hlkimage{3cm}{kibana-discover.png}


{\bf Hints:}\\
Make sure you use the right index pattern. Within discover you can change between them with the menu:

\hlkimage{3cm}{kibana-change-index-pattern.png}

Another hint, for lab systems that are turned off and on, use the time selector at the top and select "Last 90 days". This way you can see older data you imported last time. Production systems continually produce data, but your lab server probably has very little.

\hlkimage{7cm}{kibana-90-day-lab.png}

Filebeat comes with predefined assets for parsing, indexing, and visualizing your data. To load these assets run the filebeat setup command:

\begin{minted}[fontsize=\footnotesize]{shell}
filebeat setup -e
\end{minted}

Output should be similar to this:
\begin{minted}[fontsize=\footnotesize]{shell}
Overwriting ILM policy is disabled. Set `setup.ilm.overwrite: true` for enabling.

Index setup finished.
Loading dashboards (Kibana must be running and reachable)
Loaded dashboards
Setting up ML using setup --machine-learning is going to be removed in 8.0.0. Please use the ML app instead.
See more: https://www.elastic.co/guide/en/machine-learning/current/index.html
Loaded machine learning job configurations
Loaded Ingest pipelines
\end{minted}


{\bf Solution:}\\
When you have tried the filebeat tool and seen some data you are done.

{\bf Discussion:}\\



\chapter{Working with dashboards - 30 min}
\label{ex:kibana-kts}

{\bf Objective:}\\
Get a few dashboards up and running quickly.

Try the dashboard package KTS7 locally your workstation.


{\bf Purpose:}\\
There are a lot of Dashboards available, such as:\\
\link{https://github.com/StamusNetworks/KTS7}

When learning to create something, a program, document or dashboard -- seeing examples help.

Note: these instructions are very dependent on the version of Elasticsearch and Kibana, so check you version first!
\begin{minted}[fontsize=\footnotesize]{shell}
hkj@debian-siem:~$ dpkg -l | egrep "elasticsearch|kibana"
ii  elasticsearch    7.10.0       amd64    Distributed RESTful search engine built for the cloud
ii  kibana           7.10.0       amd64    Explore and visualize your Elasticsearch data
\end{minted}

Good news are that the developers of the KTS package has done them for various versions over the years. So look for KTS6 if you are running ES 6.

{\bf Suggested method:}\\
Read instructions for installing the dashboards.

\link{https://github.com/StamusNetworks/KTS7}

\begin{quote}
Kibana 7 Templates for Suricata
Templates/Dashboards for Kibana 7 to use with Suricata. Suricata IDPS/NSM threat hunting and the ELK 7 stack

This repository provides 28 dashboards for the Kibana 7.x and Elasticsearch 7.x for use with Suricata IDS/IPS/NSM - Intrusion Detection, Intrusion Prevention and Network Security Monitoring system

These dashboards are for use with Suricata 6+ and enabled Rust build, Elasticsearch, Logstash, Kibana 7 and comprise of more than 400 visualizations and 24 predefined searches.
\end{quote}

The commands are similar to, first clone:
\begin{minted}[fontsize=\footnotesize]{shell}
git clone https://github.com/StamusNetworks/KTS7.git
cd KTS7
\end{minted}

then load the dashboards -- JSON files:
\begin{minted}[fontsize=\footnotesize]{shell}
cd API-KIBANA7
curl -X POST "localhost:5601/api/saved_objects/_import" -H 'kbn-xsrf: true' --form file=@index-pattern.ndjson
curl -X POST "localhost:5601/api/saved_objects/_import" -H 'kbn-xsrf: true' --form file=@search.ndjson
curl -X POST "localhost:5601/api/saved_objects/_import" -H 'kbn-xsrf: true' --form file=@visualization.ndjson
curl -X POST "localhost:5601/api/saved_objects/_import" -H 'kbn-xsrf: true' --form file=@dashboard.ndjson
curl -X POST "localhost:5601/api/saved_objects/_import" -H 'kbn-xsrf: true' --form file=@query.ndjson
\end{minted}

Dont forget to restart Kibana
\begin{minted}[fontsize=\footnotesize]{shell}
sudo service kibana restart
\end{minted}


{\bf Hints:}\\
Kibana and ES are controlled with web requests, so if you look into the commands, they do HTTP POST requests with JSON files. This allow development of dashboards on one system, and then deploying them easily on others.

{\bf Solution:}\\
When you have tried the tool and seen some dashboards you are done.

Note: we might not have any data.

{\bf Discussion:}\\
Since these can be imported using scripts it is very easy to develop dashboards and then install them into production, staging them as code.



\end{document}

\chapter{  XX min}
\label{ex:dateformats}

%\hlkimage{10cm}{kali-linux.png}

{\bf Objective:}\\

{\bf Purpose:}\\

{\bf Suggested method:}\\


{\bf Hints:}\\

{\bf Solution:}\\
When you have a

{\bf Discussion:}\\
